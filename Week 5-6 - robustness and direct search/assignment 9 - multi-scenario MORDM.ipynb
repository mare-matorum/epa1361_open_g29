{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Scenario MORDM\n",
    "\n",
    "Multi-scenario MORMD is an extension of normal MORDM to better include robustness considerations within the search phase. It starts from the scenario discovery results resulting from MORDM. Next, from the experiments within this box, a set of scenarios is selected. \n",
    "\n",
    "There are many ways of selecting the additional scenarios. The original paper which introduced multi-scenario MORMD [Watson and Kaspzryk (2017)](https://doi.org/10.1016/j.envsoft.2016.12.001) did it in a more or less adhoc manner. [Eker and Kwakkel (2018)](https://doi.org/10.1016/j.envsoft.2018.03.029) introduced a more formal selection approach, the code of which can be found on [GitHub](https://github.com/sibeleker/MORDM---Multi-scenario-search). \n",
    "\n",
    "For this assignment, make an informed selection of 4 scenarios, using an approach of your choice. Motivate carefully your selection procedure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ema_workbench.DefaultDesigns, 1 designs on 5 parameters\n"
     ]
    }
   ],
   "source": [
    "from dps_lake_model import lake_model\n",
    "from ema_workbench import (Model, RealParameter, ScalarOutcome,\n",
    "                           MultiprocessingEvaluator, ema_logging,\n",
    "                           Constant, Constraint)\n",
    "from ema_workbench.em_framework import samplers\n",
    "from ema_workbench.em_framework import sample_levers\n",
    "from ema_workbench.em_framework.optimization import (HyperVolume,\n",
    "                                                     EpsilonProgress)\n",
    "\n",
    "# Instantiate the model\n",
    "lake_model = Model('lakeproblem', function=lake_model)\n",
    "lake_model.time_horizon = 100 # used to specify the number of timesteps\n",
    "\n",
    "# Specify uncertainties\n",
    "lake_model.uncertainties = [RealParameter('mean', 0.01, 0.05),\n",
    "                            RealParameter('stdev', 0.001, 0.005),\n",
    "                            RealParameter('b', 0.1, 0.45),\n",
    "                            RealParameter('q', 2.0, 4.5),\n",
    "                            RealParameter('delta', 0.93, 0.99)]\n",
    "\n",
    "lake_model.levers = [RealParameter('c1', -2, 2),\n",
    "                            RealParameter('c2', -2, 2),\n",
    "                            RealParameter('r1', 0, 2),\n",
    "                            RealParameter('r2', 0, 2),\n",
    "                            RealParameter('w1', 0, 1)]\n",
    "# Specify outcomes\n",
    "lake_model.outcomes = [ScalarOutcome('max_P', kind=ScalarOutcome.MINIMIZE, expected_range=(0,5)), #the bounds on max_P are not known up front. introduce a constraint saying that max_P must be below 5.\n",
    "                       ScalarOutcome('utility', kind=ScalarOutcome.MAXIMIZE,expected_range=(0,2)),\n",
    "                       ScalarOutcome('inertia', kind=ScalarOutcome.MAXIMIZE,expected_range=(0,1)),\n",
    "                       ScalarOutcome('reliability', kind=ScalarOutcome.MAXIMIZE,expected_range=(0,1))]\n",
    "\n",
    "# Run scenario\n",
    "# Generate policies by sampling over the levers (generate 5 sample levers)\n",
    "test_policies = sample_levers(lake_model,n_samples=5)\n",
    "nr_experiments = 1000\n",
    "\n",
    "print(test_policies)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario selection\n",
    "\n",
    "Possible criteria to select a small number of scenarios from a large set are internal consistency, diversity of outcome indicators, extremeness, and policy relevance (Trutnevyte et al., 2016). We would like to use the formal selection approach from Eker and Kwakkel (2018), which selects maximum diverse scenarios.\n",
    "\n",
    "The following code is partially based on https://github.com/sibeleker/MORDM---Multi-scenario-search/blob/master/MORDM_paper/MORDM%20with%20multi-scenario%20search-Rev_v1.ipynb\n",
    "\n",
    "However, there might be a misconception (from Eker and Kwakkel (2018), we assumed that the scenario selection takes place before running MORDM, as it is the first step in the MORDM with multi-scenario search(DPR).ipynb workbook).\n",
    "\n",
    "*We assume the correct order is:*\n",
    "\n",
    "* Select four scenarios from the output of Assignment 8, Step 3 (Re-evaluate candidate solutions under uncertainty).\n",
    "* The scenarios from this re-evaluation of candidate solutions are then used to find new policies taking into account the uncertainties of the scenarios.\n",
    "* Given the above applies, does it matter which policy has lead to the scenario, or are only the uncertainties relevant to establish a scenario in this context, and new policies are found in the second round of MORDM?\n",
    "\n",
    "\n",
    "## Specify model, run N=1000 scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "perform_experiments() got multiple values for argument 'policies'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-fec0da8bcd3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mMultiprocessingEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlake_model\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperform_experiments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlake_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnr_experiments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_policies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: perform_experiments() got multiple values for argument 'policies'"
     ]
    }
   ],
   "source": [
    "with MultiprocessingEvaluator(lake_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(lake_model, nr_experiments, policies=test_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not understand the error. Based on https://emaworkbench.readthedocs.io/en/latest/ema_documentation/em_framework/evaluators.html, the policies argument expects either int or collection of Policy instances. In this case, policies is a collection of Policy instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select policy relevant scenarios based on exploration results\n",
    "\n",
    "The requirements are taken from Eker and Kwakkel (2018).\n",
    "* max_P >= median(~ 5)\n",
    "* reliability <= median (~0.5)\n",
    "* inertia <= median (~1)\n",
    "* utility <= median (~0.15)\n",
    "\n",
    "Only scenarios above the outcome median should be selected. Otherwise, they are not considered relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-557a5f9e373c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Based on https://github.com/sibeleker/MORDM---Multi-scenario-search/blob/master/MORDM_paper/MORDM%20with%20multi-scenario%20search-Rev_v1.ipynb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mexperiments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutcomes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0moois\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutcomes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Based on https://github.com/sibeleker/MORDM---Multi-scenario-search/blob/master/MORDM_paper/MORDM%20with%20multi-scenario%20search-Rev_v1.ipynb\n",
    "\n",
    "experiments, outcomes = results\n",
    "oois = sorted(outcomes.keys())\n",
    "\n",
    "#here, the policy-relevant scenarios defined by median thresholds are selected\n",
    "indices = []\n",
    "for ooi in oois:\n",
    "    if ooi in ['max_P', 'inertia']:\n",
    "        a = outcomes[ooi] > np.median(outcomes[ooi])     \n",
    "    else: \n",
    "        a = outcomes[ooi] < np.median(outcomes[ooi])\n",
    "    indices.append(a)\n",
    "indices = np.swapaxes(indices, 0, 1)\n",
    "logical_index = np.array([index.all() for index in indices])\n",
    "newExperiments = experiments[logical_index]\n",
    "newOutcomes = {}\n",
    "for ooi in oois:\n",
    "    newOutcomes[ooi] = outcomes[ooi][logical_index]\n",
    "newResults = newExperiments, newOutcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find four maximimally diverse scenario in the subset of policy relevant scenarios\n",
    "This is performed by exhaustive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://github.com/sibeleker/MORDM---Multi-scenario-search/blob/master/MORDM_paper/MORDM%20with%20multi-scenario%20search-Rev_v1.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import copy\n",
    "#0 : non policy-relevant scenarios\n",
    "#1 : policy-relevant scenarios\n",
    "#2 : prim results\n",
    "#3 : diverse SELECTED\n",
    "#4 : random selected\n",
    "\n",
    "sel_column = logical_index.astype(int)\n",
    "#selected = [153, 160, 197, 207] #========SELECTED=================\n",
    "selected = [77,  96, 130, 181]\n",
    "random_selected = [81, 289, 391, 257]\n",
    "count = 0\n",
    "for index, i in enumerate(sel_column):\n",
    "    \n",
    "    if prim_logical_index[index]: #this is computed at the bottom of  the notebook\n",
    "        sel_column[index] = 2\n",
    "    if i:\n",
    "        if count in selected:\n",
    "            sel_column[index] = 3\n",
    "        count +=1 #the reason for the count is that the selected indices correspond to the dataset of 206 scenarios \n",
    "            \n",
    "    if index in random_selected:\n",
    "        sel_column[index] = 4\n",
    "        \n",
    "\n",
    "print(len(sel_column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for each scenario\n",
    "\n",
    "For each of the four selected scenarios, use many-objective optimization to find a pareto approximate set using the same approach as for assignment 8. Remember to check for convergence (and time permitting, seed analysis), and be careful in what epsilon values to use (not to coarse, not too small). \n",
    "\n",
    "Store the resulting set of pareto solutions in a smart way for subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-evaluate under deep uncertainty\n",
    "\n",
    "Combine the pareto set of solutions found for each scenario. Next, turn each solution into a policy object. If you have a very large number of policies, you can choose to down sample your policies in some reasoned way (*e.g.*, picking min and max on each objective, slicing across the pareto front with a particular step size). As a rule of thumb, try to limit the set of policies to at most 50. \n",
    "\n",
    "Re-evaluate the combined set of solutions over 1000 scenarios sampled using LHS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate both the maximum regret, and the domain criterion using the values provided in [Bartholomew and Kwakkel (2020)](https://doi.org/10.1016/j.envsoft.2020.104699). Ignore the max_P objective.\n",
    "\n",
    "visualize the results in parallel coordinate plot. \n",
    "\n",
    "Are there any promising compromise solutions which balance performance in both the reference scenarios as well as in terms of their robustness?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
